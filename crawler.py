import urllib.error
import urllib.parse
import urllib.request

from bs4 import BeautifulSoup
from attack import attacker
from utils import webSeacher, makeRequest, writeToFile


# Reference: https://www.thepythoncode.com/article/extract-all-website-links-python
# Reference: https://www.scrapingbee.com/blog/crawling-python/

def crawler(targetUrl, report_path, vulnerabilities):
    new_file = report_path + "/crawl.csv"  # create a new csv file
    seed_file = open(new_file, "w+")  # create a new file
    seed_file.writelines("{},{},{},{}\n".format("website", "host", "method: GET/POST", " params "))
    original_website = webSeacher(targetUrl)
    crawled_urls, crawled_forms = [], []
    html = urllib.request.urlopen(targetUrl).read()
    soup = BeautifulSoup(html, "html.parser")
    bs_href = soup.find_all("a")
    bs_form = soup.find_all("form")
    next_link = ""

    for a_tag in bs_href:  # find all the links
        anchor = a_tag.get("href")  # get the href attribute
        if anchor.find("#") != -1:  # -1 means not found
            anchor = anchor.split("#")[0]  # split the line by # and get the first element
        try:
            if anchor.startswith("http"):
                if anchor not in crawled_urls:  # if the url is not in the crawled_urls list
                    crawled_urls.append(anchor)  # add the url to the crawled_urls list
            elif anchor[:1] == "/":
                url_data = original_website + anchor  # add the absolute website link
                if url_data not in crawled_urls:
                    crawled_urls.append(url_data)
        except TypeError:
            pass

    # looping through the urls
    for url in crawled_urls:
        if url.startswith(original_website):
            try:
                urllib.request.urlopen(url).read()
                print(url)
            except urllib.error.HTTPError as http_error:
                pass
            for line in bs_href:
                href_line = line.get('href')
                if href_line and href_line.find("#") != -1:
                    href_line = href_line.split("#")[0]
                try:
                    if href_line[:4] == "http":
                        if href_line not in crawled_forms:
                            crawled_forms.append(href_line)
                    elif href_line.startswith("/"):
                        temp = original_website + href_line
                        if temp not in crawled_forms:
                            crawled_forms.append(original_website + href_line)
                except TypeError:
                    pass
    # looping through the forms
    for form_list in crawled_forms:
        if form_list.startswith(original_website):
            if form_list.find("#") != -1:
                form_list = form_list.split("#")[0]
            if form_list.find("=") != -1:
                para = form_list[form_list.find("?") + 1: form_list.find("=")]
                seed_file.writelines("{},{},{},{}\n".format(form_list, form_list.split("?")[0], "GET", para))
            if bs_form:
                for form in bs_form:
                    value_list = []
                    method = form.get("method")
                    if method.lower() == "get":
                        action = form.get("action")
                        if action.startswith("/") == 0:
                            next_link = form_list.split("?")[0] + str(action)
                        else:
                            next_link = original_website + str(action)
                        for inp_tag in form.find_all("input"):
                            if inp_tag.get("type").lower() == "submit":
                                pass
                            else:
                                name_tag = inp_tag.get("name")
                                value_list.append(name_tag)
                        seed_file.writelines("{},{},{}".format(form_list, next_link, "GET"))
                        for val in value_list:
                            seed_file.writelines(",{}".format(val))
                    elif method.lower() == "post":
                        action = form.get("action")
                        if action.startswith("/") == 0:
                            next_link = form_list.split("?")[0] + str(action)
                        else:
                            next_link = original_website + str(action)
                        for inp_tag in form.find_all("input"):
                            if inp_tag.get("type").lower() == "submit":
                                pass
                            else:
                                name_tag = inp_tag.get("name")
                                value_list.append(name_tag)
                            if "?" in action:  # if the action contains ? = parameters
                                action = action.split("?")[0]
                        seed_file.writelines("{},{},{}".format(form_list, next_link, "POST"))
                        for val in value_list:
                            seed_file.writelines(",{}".format(val))
                    seed_file.writelines("\n")
    seed_file.close()
    attacker(new_file, report_path, vulnerabilities)
